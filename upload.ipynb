{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: /home/ubuntu/lora/saved_models_2pow14/gemma-2-2b_standard_new_width-2pow14_date-0107/resid_post_layer_12/trainer_1/peft_sae_full_finetune/rank_16/ae.pt does not exist.\n",
      "Removed directory: /home/ubuntu/lora/saved_models_2pow14/gemma-2-2b_standard_new_width-2pow14_date-0107/resid_post_layer_12/trainer_1/peft_sae_full_finetune\n",
      "Created config file at: /home/ubuntu/lora/saved_models_2pow14/gemma-2-2b_standard_new_width-2pow14_date-0107/resid_post_layer_12/trainer_1/config.json\n",
      "Warning: /home/ubuntu/lora/saved_models_2pow14/gemma-2-2b_standard_new_width-2pow14_date-0107/resid_post_layer_12/trainer_5/peft_sae_full_finetune/rank_16/ae.pt does not exist.\n",
      "Removed directory: /home/ubuntu/lora/saved_models_2pow14/gemma-2-2b_standard_new_width-2pow14_date-0107/resid_post_layer_12/trainer_5/peft_sae_full_finetune\n",
      "Created config file at: /home/ubuntu/lora/saved_models_2pow14/gemma-2-2b_standard_new_width-2pow14_date-0107/resid_post_layer_12/trainer_5/config.json\n",
      "Warning: /home/ubuntu/lora/saved_models_2pow14/gemma-2-2b_standard_new_width-2pow14_date-0107/resid_post_layer_12/trainer_4/peft_sae_full_finetune/rank_16/ae.pt does not exist.\n",
      "Removed directory: /home/ubuntu/lora/saved_models_2pow14/gemma-2-2b_standard_new_width-2pow14_date-0107/resid_post_layer_12/trainer_4/peft_sae_full_finetune\n",
      "Created config file at: /home/ubuntu/lora/saved_models_2pow14/gemma-2-2b_standard_new_width-2pow14_date-0107/resid_post_layer_12/trainer_4/config.json\n",
      "Warning: /home/ubuntu/lora/saved_models_2pow14/gemma-2-2b_standard_new_width-2pow14_date-0107/resid_post_layer_12/trainer_0/peft_sae_full_finetune/rank_16/ae.pt does not exist.\n",
      "Removed directory: /home/ubuntu/lora/saved_models_2pow14/gemma-2-2b_standard_new_width-2pow14_date-0107/resid_post_layer_12/trainer_0/peft_sae_full_finetune\n",
      "Created config file at: /home/ubuntu/lora/saved_models_2pow14/gemma-2-2b_standard_new_width-2pow14_date-0107/resid_post_layer_12/trainer_0/config.json\n",
      "Warning: /home/ubuntu/lora/saved_models_2pow14/gemma-2-2b_standard_new_width-2pow14_date-0107/resid_post_layer_12/trainer_2/peft_sae_full_finetune/rank_16/ae.pt does not exist.\n",
      "Removed directory: /home/ubuntu/lora/saved_models_2pow14/gemma-2-2b_standard_new_width-2pow14_date-0107/resid_post_layer_12/trainer_2/peft_sae_full_finetune\n",
      "Created config file at: /home/ubuntu/lora/saved_models_2pow14/gemma-2-2b_standard_new_width-2pow14_date-0107/resid_post_layer_12/trainer_2/config.json\n",
      "Warning: /home/ubuntu/lora/saved_models_2pow14/gemma-2-2b_standard_new_width-2pow14_date-0107/resid_post_layer_12/trainer_3/peft_sae_full_finetune/rank_16/ae.pt does not exist.\n",
      "Removed directory: /home/ubuntu/lora/saved_models_2pow14/gemma-2-2b_standard_new_width-2pow14_date-0107/resid_post_layer_12/trainer_3/peft_sae_full_finetune\n",
      "Created config file at: /home/ubuntu/lora/saved_models_2pow14/gemma-2-2b_standard_new_width-2pow14_date-0107/resid_post_layer_12/trainer_3/config.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "trainer_l1_penalty_mapping = {\n",
    "    0: 0.012,\n",
    "    1: 0.015,\n",
    "    2: 0.02,\n",
    "    3: 0.03,\n",
    "    4: 0.04,\n",
    "    5: 0.06\n",
    "}\n",
    "\n",
    "# Assume current directory is the one containing \"trainer_*\" folders.\n",
    "base_dir = os.getcwd()\n",
    "\n",
    "# Iterate over each trainer folder\n",
    "for trainer_folder in os.listdir(base_dir):\n",
    "    trainer_path = os.path.join(base_dir, trainer_folder)\n",
    "    if os.path.isdir(trainer_path) and trainer_folder.startswith(\"trainer_\"):\n",
    "        try:\n",
    "            trainer_id = int(trainer_folder.split(\"_\")[-1])\n",
    "        except ValueError:\n",
    "            print(f\"Skipping {trainer_folder}: unable to parse trainer ID.\")\n",
    "            continue\n",
    "\n",
    "        # Path where ae.pt is currently stored inside the redundant folder structure.\n",
    "        src_ae = os.path.join(trainer_path, \"peft_sae_full_finetune\", \"rank_1\", \"ae.pt\")\n",
    "        # New destination: directly under the trainer folder.\n",
    "        dst_ae = os.path.join(trainer_path, \"ae.pt\")\n",
    "\n",
    "        if os.path.exists(src_ae):\n",
    "            # Move ae.pt from its old location to the trainer folder.\n",
    "            shutil.move(src_ae, dst_ae)\n",
    "            print(f\"Moved: {src_ae} -> {dst_ae}\")\n",
    "        else:\n",
    "            print(f\"Warning: {src_ae} does not exist.\")\n",
    "\n",
    "        # Remove the redundant folder structure.\n",
    "        redundant_dir = os.path.join(trainer_path, \"peft_sae_full_finetune\")\n",
    "        if os.path.exists(redundant_dir):\n",
    "            shutil.rmtree(redundant_dir)\n",
    "            print(f\"Removed directory: {redundant_dir}\")\n",
    "\n",
    "        # Build the config with the correct \"k\" value.\n",
    "\n",
    "        config = {\n",
    "            \"trainer\": {\n",
    "                \"dict_class\": \"AutoEncoder\",\n",
    "                \"trainer_class\": \"StandardTrainerAprilUpdate\",\n",
    "                \"activation_dim\": 2304,\n",
    "                \"dict_size\": 16384,\n",
    "                \"lr\": 0.0003,\n",
    "                \"l1_penalty\": trainer_l1_penalty_mapping[trainer_id],\n",
    "                \"warmup_steps\": 1000,\n",
    "                \"sparsity_warmup_steps\": 5000,\n",
    "                \"steps\": 244140,\n",
    "                \"decay_start\": 195312,\n",
    "                \"seed\": 0,\n",
    "                \"device\": \"cuda:2\",\n",
    "                \"layer\": 12,\n",
    "                \"lm_name\": \"google/gemma-2-2b\",\n",
    "                \"wandb_name\": f\"StandardTrainerNew-google/gemma-2-2b-resid_post_layer_12_trainer_{trainer_id}\",\n",
    "                \"submodule_name\": \"resid_post_layer_12\"\n",
    "            },\n",
    "            \"buffer\": {\n",
    "                \"d_submodule\": 2304,\n",
    "                \"io\": \"out\",\n",
    "                \"n_ctxs\": 244,\n",
    "                \"ctx_len\": 1024,\n",
    "                \"refresh_batch_size\": 128,\n",
    "                \"out_batch_size\": 2048,\n",
    "                \"device\": \"cuda:2\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "        # Write the config.json file next to the ae.pt file.\n",
    "        config_path = os.path.join(trainer_path, \"config.json\")\n",
    "        with open(config_path, \"w\") as f:\n",
    "            json.dump(config, f, indent=4)\n",
    "        print(f\"Created config file at: {config_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
