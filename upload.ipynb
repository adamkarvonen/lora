{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "trainer_l1_penalty_mapping = {\n",
    "    0: 0.012,\n",
    "    1: 0.015,\n",
    "    2: 0.02,\n",
    "    3: 0.03,\n",
    "    4: 0.04,\n",
    "    5: 0.06\n",
    "}\n",
    "\n",
    "dict_size = 16384\n",
    "# dict_size = 65536\n",
    "\n",
    "if dict_size == 16384:\n",
    "    size = \"2pow14\"\n",
    "elif dict_size == 65536:\n",
    "    size = \"2pow16\"\n",
    "else:\n",
    "    raise ValueError(f\"Invalid dict size: {dict_size}\")\n",
    "\n",
    "# Assume current directory is the one containing \"trainer_*\" folders.\n",
    "base_dir = \"gemma-2-2b\"\n",
    "\n",
    "# Iterate over each trainer folder\n",
    "for trainer_folder in os.listdir(base_dir):\n",
    "    trainer_path = os.path.join(base_dir, trainer_folder)\n",
    "    if os.path.isdir(trainer_path) and trainer_folder.startswith(\"trainer_\"):\n",
    "        try:\n",
    "            trainer_id = int(trainer_folder.split(\"_\")[-1])\n",
    "        except ValueError:\n",
    "            print(f\"Skipping {trainer_folder}: unable to parse trainer ID.\")\n",
    "            continue\n",
    "\n",
    "        # Path where ae.pt is currently stored inside the redundant folder structure.\n",
    "        src_ae = os.path.join(trainer_path, \"peft_sae_full_finetune\", \"rank_1\", \"ae.pt\")\n",
    "        # New destination: directly under the trainer folder.\n",
    "        dst_ae = os.path.join(trainer_path, \"ae.pt\")\n",
    "\n",
    "        if os.path.exists(src_ae):\n",
    "            # Move ae.pt from its old location to the trainer folder.\n",
    "            shutil.move(src_ae, dst_ae)\n",
    "            print(f\"Moved: {src_ae} -> {dst_ae}\")\n",
    "        else:\n",
    "            raise ValueError(f\"Source file {src_ae} does not exist.\")\n",
    "\n",
    "        # Remove the redundant folder structure.\n",
    "        redundant_dir = os.path.join(trainer_path, \"peft_sae_full_finetune\")\n",
    "        if os.path.exists(redundant_dir):\n",
    "            shutil.rmtree(redundant_dir)\n",
    "            print(f\"Removed directory: {redundant_dir}\")\n",
    "\n",
    "        # Build the config with the correct \"k\" value.\n",
    "\n",
    "        config = {\n",
    "            \"trainer\": {\n",
    "                \"dict_class\": \"AutoEncoder\",\n",
    "                \"trainer_class\": \"StandardTrainerAprilUpdate\",\n",
    "                \"activation_dim\": 2304,\n",
    "                \"dict_size\": dict_size,\n",
    "                \"lr\": 0.0003,\n",
    "                \"l1_penalty\": trainer_l1_penalty_mapping[trainer_id],\n",
    "                \"warmup_steps\": 1000,\n",
    "                \"sparsity_warmup_steps\": 5000,\n",
    "                \"steps\": 244140,\n",
    "                \"decay_start\": 195312,\n",
    "                \"seed\": 0,\n",
    "                \"device\": \"cuda:0\",\n",
    "                \"layer\": 12,\n",
    "                \"lm_name\": \"google/gemma-2-2b\",\n",
    "                \"wandb_name\": f\"StandardTrainerNew-google/gemma-2-2b-resid_post_layer_12_trainer_{trainer_id}\",\n",
    "                \"submodule_name\": \"resid_post_layer_12\"\n",
    "            },\n",
    "            \"buffer\": {\n",
    "                \"d_submodule\": 2304,\n",
    "                \"io\": \"out\",\n",
    "                \"n_ctxs\": 244,\n",
    "                \"ctx_len\": 1024,\n",
    "                \"refresh_batch_size\": 128,\n",
    "                \"out_batch_size\": 2048,\n",
    "                \"device\": \"cuda:0\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "        # Write the config.json file next to the ae.pt file.\n",
    "        config_path = os.path.join(trainer_path, \"config.json\")\n",
    "        with open(config_path, \"w\") as f:\n",
    "            json.dump(config, f, indent=4)\n",
    "        print(f\"Created config file at: {config_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "new_folder = f\"gemma-2-2b_standard_new_width-{size}_date-0107\"\n",
    "existing_folder = \"gemma-2-2b\"\n",
    "subfolder = \"resid_post_layer_12\"\n",
    "\n",
    "# Rename existing folder to subfolder\n",
    "os.rename(existing_folder, subfolder)\n",
    "\n",
    "# Create the new folder\n",
    "os.makedirs(new_folder, exist_ok=True)\n",
    "\n",
    "# Move subfolder into new_folder\n",
    "shutil.move(subfolder, os.path.join(new_folder, subfolder))\n",
    "\n",
    "print(f\"Renamed '{existing_folder}' to '{subfolder}'\")\n",
    "print(f\"Created new folder: '{new_folder}'\")\n",
    "print(f\"Moved '{subfolder}' into '{new_folder}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "api.upload_large_folder(\n",
    "    folder_path=\".\",\n",
    "    # path_in_repo=\"\",\n",
    "    repo_id=\"adamkarvonen/new_kl_finetunes\",\n",
    "    repo_type=\"model\",\n",
    "    allow_patterns=[\"*.json\", \"*.pt\"],\n",
    "    ignore_patterns=[\".DS_Store\"],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
